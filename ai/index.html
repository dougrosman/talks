<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link href="https://fonts.googleapis.com/css2?family=Work+Sans:wght@100..900&display=swap" rel="stylesheet"> 
  <link rel="stylesheet" href="libs/prism.min.css">
  <link rel="stylesheet" href="style.css">
  <script src="libs/jquery-3.5.1.min.js"></script>
  <title>AI + Teachable Machine Workshop</title>
</head>
<body>
  <div class="wrapper">
    <header>
      <h1 class="title">AI + Teachable Machine</h1>
      <h2 class="subtitle">Visiting Artist workshop w/ <a href="https://dougrosman.com" target="_blank">Doug Rosman</a></h2>
      <nav>
        <ol>
          <li><a href="#doug-rosman">Doug Rosman</a></li>
          <li><a href="#ai">What is Artificial Intelligence?</a></li>
          <li><a href="#machine-learning">What is Machine learning?</a></li>
          <li><a href="#ai-art">AI AT THE INTERSECTION OF ART AND TECHNOLOGY?</a></li>
          <li><a href="#gan">GAN Experiments</a></li>
          <li><a href="#additional-resources">Additional Resources</a></li>
          <li><a href="#teachable-machine">Teachable Machine Demo</a></li>
        </ol>
      </nav>
    </header>
    <main class="counters">
      <section id="doug-rosman">
        <h3>Doug Rosman</h3>
        <p><a href="https://dougrosman.com">dougrosman.com</a></p>
        <h4 class="work-title">self-contained</h4>
        <p>
          <a href="images/self-contained01.jpg" target="_blank"><img src="images/self-contained01.jpg" alt="self-contained 01"></a>
          <a href="images/self-contained02.jpg" target="_blank"><img src="images/self-contained02.jpg" alt="self-contained 02"></a>
          <a href="images/self-contained03.jpg" target="_blank"><img src="images/self-contained03.jpg" alt="self-contained 03"></a>
        </p>
        <p><iframe src="https://player.vimeo.com/video/331575304?color=ffffff&portrait=0" width="640" height="320" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe></p>
        <p><iframe src="https://player.vimeo.com/video/341014454?color=ffffff&portrait=0" width="640" height="320" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe></p>
        <p><iframe src="https://player.vimeo.com/video/341014803?color=ffffff&portrait=0" width="640" height="320" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe></p>
        <h4 class="work-title">Patriot Coin (work in progress)</h4>
        <p><img src="images/patriot_coin.jpg" alt="Patriot Coin"></p>
        <p><a href="https://dougrosman.github.io/PatriotCoin/" target="_blank">Link to PatriotCoin (you need a MetaMask account to and ETH on the Rinkeby Testnet to experience this work)</a></p>
      </section>
      <section id="ai">
        <h3>What is Artificial Intelligence?</h3>
      </section>
      <section id="machine-learning">
        <h3>What is Machine Learning?</h3>
        <img src="images/tasks.jpg" alt="Tasks XKCD">
        <p><a href="https://thispersondoesnotexist.com/" target="_blank"></a></p>
      </section>
      <section id="ai-art">
        <h3>AI AT THE INTERSECTION OF ART AND TECHNOLOGY?</h3>
        <p><a href="https://www.memo.tv/works/gloomy-sunday/" target="_blank">Memo Akten</a></p>
        <p><a href="https://www.instagram.com/soficrespo91/?hl=en" target="_blank">Sofia Crespo</a></p>
        <p><a href="https://annaridler.squarespace.com/recording-nature" target="_blank">Anna Ridler</a></p>
        <p><a href="https://www.instagram.com/dvsmethid/?hl=en" target="_blank">Derrick Schultz</a></p>
        <p><a href="http://quasimondo.com/" target="_blank">Mario Klingemann</a></p>
        <p><a href="https://robbiebarrat.github.io/" target="_blank">Robbie Barrat</a></p>
        <p><a href="https://www.dwbowen.com/flyai" target="_blank">David Bowen</a></p>
        <p></p>
      </section>
      <section id="gan">
        <h3>GAN Experiments</h3>
        <video src="videos/cat_gan.mp4" controls muted></video>
        <video src="videos/xray_gan.mp4" controls muted></video>
        <div class="side-by-side">
          <video src="videos/batman-projection.mp4" controls muted></video>
          <img src="images/batman.png" alt="">
        </div>
        <div class="side-by-side">
          <video src="videos/ken-projection.mp4" controls muted></video>
          <img src="images/ken.png" alt="">
        </div>
      </section>
      <section id="additional-resources">
        <h3>Additional Resources</h3>
        <p><a href="https://runwayml.com/" target="_blank">RunwayML - Machine Learning for Creators</a> - Experiment with a lots of different machine learning models.</p>
        <p><a href="https://www.youtube.com/user/bustbright">Artificial Images YouTube Channel</a> - A one-stop-shop for artists wanting to learn how to train their own GANs (for free!)</p>
      </section>
      <section id="teachable-machine">
        <h3>Teachable Machine Demo</h3>
        <p class="intro">This demo should you get you up and running with an interactive p5.js sketch that uses a Machine Learning-based image recognition model that we'll train from scratch* using Google's Teachable Machine!</p>
        <p class="ps">If you're creating a sketch outside of the p5.js web editor, you'll need to download the <a href="libs/p5.min.js" download>p5.js</a> and <a href="libs/ml5.min.js" download>ml5.js</a> libraries and put them in a folder next to your sketch.js file. The template p5.js code provided by Teachable Machine requires an older version of p5.js and ml5.js. The links here will download those older versions.</p>
        <h4 class="demo-section__title">Part 1: Training your model in Teachable Machine</h4>
        <ol class="demo-section__content">
          <li>Go to <a href="https://teachablemachine.withgoogle.com/" target="_blank">teachablemachine.withgoogle.com</a> and click 'Get Started'</li>
          <li>Select 'Image Project'</li>
          <li>Train your image model! Create and name as many classes as you'll need for your model.</li>
          <li>Change your settings for image recording by pressing Webcam, and the gear icon.<ol>
            <li><strong>Set your FPS to 15 or lower:</strong> Even though your webcam likely records at 30fps, you don't want all those images. At that high framerate, all your images tend to look very similar, which doesn't increase your data set's variety.</li>
            <li><strong>Turn off 'Hold to Record':</strong> Turning this off frees up your hands during recording. Sometimes leaving this on can be useful.</li>
            <li><strong>Give yourself a delay</strong></li>
            <li><strong>Set your duration to at least 10 seconds:</strong> You want at least 100 images per class, so set the duration to something that'll make sure you get enough images.</li>
          </ol>
          <img src="images/tm_setings.jpg" alt="Teachable machine settings">
        </li>
        <li>Record or upload images to each class. There's a whole lot to be said about what makes a "good" data set. But here are some general tips:
          <ol>
            <li><b>Quantity > Quality:</b> In general, 500 "okay" images will be better than 50 "good" images. There's no right number for how many images you should have per class, but you should have <strong>at least 100 images per class.</strong></li>
            <li><b>Variety is the spice of a well-composed data set:</b> Make sure to capture a wide variety of images for each class by altering your capture conditions. Change your lighting, move the object toward and away from the camera, position the object at different angles, record different versions of the same object (for example, if you're teaching a machine to recognize a dog, you should probably include multiple dog breeds in your data set).</li>
            <li><b>Be aware of your context:</b> Computers have a mind of their own, and will learn things you didn't mean for them to learn. For example, if you're trying to teach the machine how to recognize your drawings, keep in mind that it's paying just as much attention to the paper the drawing is on as much as the drawing itself.</li>
            <li><b>(MOST IMPORTANT) Be aware of bias!!:</b> Continuing the point above, <em>computers will learn things you didn't mean for them to learn. This can have harmful consequences</em>. For example, if you're training your model to recognize a human face, will it perform equally for people with different skin tones? If your data set is composed of images of only one kind of person, then it will only perform with the most accuracy for that type of person. We usually don't have access to every type of person (or object, or whatever it is we're filling our data set with), so it's important to acknowledge that our image models are imperfect, and might not perform well for everyone. It's okay when we're just experimenting with artistic sketches in the comfort of our homes or classrooms, but these issues become much more important when we put these tools out into the world--where we can't be sure who will be interacting with them.</li>
            </ol></li>
            <p class="ps">Bias in AI is one of the most important issues facing the proliferation of this technology. There is no such thing as a "neutral" AI, and the issue of bias in machine learning is something that can never be "solved" (though that doesn't mean we shouldn't do everything we can to understand it and work to remove it). Many companies that use machine learning in their product stack have (or <a href="https://www.theverge.com/2019/4/4/18296113/google-ai-ethics-board-ends-controversy-kay-coles-james-heritage-foundation" target="_blank">weakly gestured toward having...</a>) something like an AI ethics division, whose responsibility it is to make sure the AI tools they put out into the world are as ethical and unbiased as they possibly can be (within reason for them to remain profitable, anyway...spending time).</p>
            <li>Once you've recorded all the images you want for you class, click 'Train Model.' Make sure to leave your tab open while training. You can open a new window in your browser if you want to keep browsing.</li>
            <li>Once training completes, click 'Export Model'</li>
            <li>For convenience, click 'Upload my model.' <em>Make sure to save the link that is generated once it is uploaded!</em></li>
            <li>Below where it says 'Code snippets', click 'p5.js', then click 'p5.js Web Editor' where it says 'Open up the code snippet below directly in the p5.js Web Editor.</li>
            <img src="images/tm_export.jpg" alt="Teachable machine Export">
            <li>In the p5 web editor, swap out the model link (line 15) with your own link. (Doug's pretrained hand-direction model:)</li>
            <pre><code class="language-javascript">https://teachablemachine.withgoogle.com/models/mLR2k5Qif/model.json</code></pre>
            <li>Doug's p5.js demo sketches: <a href="https://editor.p5js.org/drosman-cvml/sketches/o_3zJx0Rq" target="_blank"><em>Hand Direction 1</em></a>, <a href="https://editor.p5js.org/drosman-cvml/sketches/MAxgGbAPz" target="_blank"><em>Hand Direction 2: Granular Confidence</em></a></li>
            <img src="images/tm_model.jpg" alt="Teachable machine Export">
            <li>Here's the template code for the Teachable Machine Image Recognition model. You can find this code here: <a href="https://editor.p5js.org/ml5/sketches/ImageModel_TM" target="_blank">ml5 Example
              Webcam Image Classification</a> Before doing anything, try playing the sketch to make sure it works. You'll need to make sure your webcam isn't occupied by another program (like Zoom, or the other tab running Teachable Machine)</li>

        
        <pre><code class="language-javascript">// Copyright (c) 2019 ml5
//
// This software is released under the MIT License.
// https://opensource.org/licenses/MIT

/* ===
ml5 Example
Webcam Image Classification using a pre-trained customized model and p5.js
This example uses p5 preload function to create the classifier
=== */

// Classifier Variable
let classifier;
// Model URL
let imageModelURL =
'https://teachablemachine.withgoogle.com/models/bXy2kDNi/model.json';

// Video
let video;
let flippedVideo;
// To store the classification
let label = "";
let confidence = 0; // Make sure to add this variable!

// Load the model first
function preload() {
  classifier = ml5.imageClassifier(imageModelURL);
}

function setup() {
  createCanvas(320, 260);
  // Create the video
  video = createCapture(VIDEO);
  video.size(320, 240);
  video.hide();

  flippedVideo = ml5.flipImage(video)
  // Start classifying
  classifyVideo();
}

function draw() {
  background(0);
  // Draw the video
  image(flippedVideo, 0, 0);

  // Draw the label
  fill(255);
  textSize(16);
  textAlign(CENTER);
  text(label, width / 2, height - 4);
}

// Get a prediction for the current video frame
function classifyVideo() {
  flippedVideo = ml5.flipImage(video)
  classifier.classify(flippedVideo, gotResult);
}

// When we get a result
function gotResult(error, results) {
  // If there is an error
  if (error) {
    console.error(error);
    return;
  }
  // The results are in an array ordered by confidence.
  // console.log(results[0]);
  label = results[0].label;
  confidence = results[0].confidence;
  // make sure you added 'let confidence = 0' to the top of your code!
  
  // Classifiy again!
  classifyVideo();
}</code></pre>
<li>There isn't much you have to do to customize this code to make use of the image recognition data! There are two pieces of information we can get from the classifier: <ol>
  <li><strong>Label:</strong> The label is the name of your class. results[0].label will return the label the classifier feels most confident about.</li>
  <li><strong>Confidence: </strong> The confidence is how sure the classifier is about what it sees. This number ranges from 0-1, where 1 means 100% confidence in identifying the object.</li>
</ol></li>
<li>Let's take a look at the 'gotResult' function. This is where we can work with the data being returned by the classifier to control elements of our sketch. We can change some of the code to access the confidence and labels that we want.</li>
<pre><code class="language-javascript">// When we get a result
function gotResult(error, results) {
  // If there is an error
  if (error) {
    console.error(error);
    return;
  }
  // The results are in an array ordered by confidence.
  // console.log(results[0]);
  label = results[0].label;
  confidence = results[0].confidence;
  // Classifiy again!
  classifyVideo();
}
</code></pre>
<li>If you wanted to control certain actions with different labels and confidence values, you can use if-statements:</li>
<pre><code class="language-javascript">  label = results[0].label;
  confidence = results[0].confidence;

  if (label == "Move Up") {
    // move circle Up (decrease Y value)
  } else if (label == "Move Right") {
    // move circle Right (increase X value)
  } else if (label == "Move Down") {
    // move circle Down (increase Y value)
  } else if (label == "Move Left") {
    // move circle Left (decrease X value)
  } else {
    // do nothing
  }
  // Classifiy again!
  classifyVideo();</code></pre>
  <li>You could also do something like use the confidence value to determine the size of the circle you're using to draw:</li>
  <pre><code class="language-javascript">  confidence = results[0].confidence;
  radius = map(confidence, 0, 1, 1, 100);
  /* if you want to use radius, you would first have
   to declare it at the top of your code */</code></pre>
</ol>
      </section>
    </main>
    
  </div>

  <script src="libs/prism.min.js"></script>
  <script src="utils.js"></script>
</body>
</html>